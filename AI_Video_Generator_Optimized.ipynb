{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üé¨ AI Video Generator - Optimized for Google Colab\n",
    "\n",
    "**Features:**\n",
    "- 720p Resolution Support\n",
    "- 10-second video generation\n",
    "- 24fps output\n",
    "- Realistic human posing\n",
    "- Batch processing\n",
    "- Memory optimization\n",
    "- Progress tracking\n",
    "- Automatic environment detection\n",
    "\n",
    "**Requirements:**\n",
    "- GPU Runtime (T4 or better recommended)\n",
    "- ~15GB RAM\n",
    "- ~20GB Disk Space\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## üìã Step 1: Environment Detection & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "environment_detection"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import platform\n",
    "import psutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing optional packages with fallbacks\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    print(\"‚úÖ ipywidgets imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ipywidgets not available, some UI features may be limited\")\n",
    "    \n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    print(\"‚úÖ tqdm imported successfully\")\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "    print(\"‚ö†Ô∏è Using console tqdm instead of notebook version\")\n",
    "\n",
    "class EnvironmentDetector:\n",
    "    def __init__(self):\n",
    "        self.system_info = {}\n",
    "        self.requirements_met = False\n",
    "        \n",
    "    def detect_environment(self):\n",
    "        \"\"\"Detect and validate the current environment\"\"\"\n",
    "        print(\"üîç Detecting environment...\")\n",
    "        \n",
    "        # Basic system info\n",
    "        self.system_info['platform'] = platform.system()\n",
    "        self.system_info['python_version'] = sys.version\n",
    "        self.system_info['is_colab'] = 'google.colab' in sys.modules\n",
    "        \n",
    "        # Memory info\n",
    "        memory = psutil.virtual_memory()\n",
    "        self.system_info['total_ram_gb'] = round(memory.total / (1024**3), 2)\n",
    "        self.system_info['available_ram_gb'] = round(memory.available / (1024**3), 2)\n",
    "        \n",
    "        # GPU info with fallback\n",
    "        try:\n",
    "            import torch\n",
    "            self.system_info['cuda_available'] = torch.cuda.is_available()\n",
    "            if torch.cuda.is_available():\n",
    "                self.system_info['gpu_name'] = torch.cuda.get_device_name(0)\n",
    "                self.system_info['gpu_memory_gb'] = round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)\n",
    "            else:\n",
    "                self.system_info['gpu_name'] = 'None'\n",
    "                self.system_info['gpu_memory_gb'] = 0\n",
    "        except:\n",
    "            self.system_info['cuda_available'] = False\n",
    "            self.system_info['gpu_name'] = 'None'\n",
    "            self.system_info['gpu_memory_gb'] = 0\n",
    "        \n",
    "        # Disk space\n",
    "        disk_usage = psutil.disk_usage('/')\n",
    "        self.system_info['disk_free_gb'] = round(disk_usage.free / (1024**3), 2)\n",
    "        \n",
    "        self._display_info()\n",
    "        self._check_requirements()\n",
    "        \n",
    "    def _display_info(self):\n",
    "        \"\"\"Display system information in a nice format\"\"\"\n",
    "        info_html = f\"\"\"\n",
    "        <div style=\"background-color: #f0f2f6; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "            <h3>üñ•Ô∏è System Information</h3>\n",
    "            <table style=\"width: 100%; border-collapse: collapse;\">\n",
    "                <tr><td><b>Platform:</b></td><td>{self.system_info['platform']}</td></tr>\n",
    "                <tr><td><b>Google Colab:</b></td><td>{'‚úÖ Yes' if self.system_info['is_colab'] else '‚ùå No'}</td></tr>\n",
    "                <tr><td><b>Total RAM:</b></td><td>{self.system_info['total_ram_gb']} GB</td></tr>\n",
    "                <tr><td><b>Available RAM:</b></td><td>{self.system_info['available_ram_gb']} GB</td></tr>\n",
    "                <tr><td><b>GPU:</b></td><td>{self.system_info['gpu_name']}</td></tr>\n",
    "                <tr><td><b>GPU Memory:</b></td><td>{self.system_info['gpu_memory_gb']} GB</td></tr>\n",
    "                <tr><td><b>Free Disk Space:</b></td><td>{self.system_info['disk_free_gb']} GB</td></tr>\n",
    "            </table>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(HTML(info_html))\n",
    "        \n",
    "    def _check_requirements(self):\n",
    "        \"\"\"Check if system meets requirements\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        if not self.system_info['cuda_available']:\n",
    "            issues.append(\"‚ùå CUDA not available - GPU required for optimal performance\")\n",
    "        elif self.system_info['gpu_memory_gb'] < 6:\n",
    "            issues.append(\"‚ö†Ô∏è GPU memory < 6GB - may experience memory issues\")\n",
    "            \n",
    "        if self.system_info['total_ram_gb'] < 12:\n",
    "            issues.append(\"‚ö†Ô∏è RAM < 12GB - may experience memory issues\")\n",
    "            \n",
    "        if self.system_info['disk_free_gb'] < 15:\n",
    "            issues.append(\"‚ö†Ô∏è Disk space < 15GB - may not have enough space for models\")\n",
    "            \n",
    "        if issues:\n",
    "            print(\"\\n‚ö†Ô∏è System Issues Detected:\")\n",
    "            for issue in issues:\n",
    "                print(f\"  {issue}\")\n",
    "            print(\"\\nüí° Recommendation: Use Google Colab Pro with GPU runtime for best results\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ All requirements met! Ready to proceed.\")\n",
    "            self.requirements_met = True\n",
    "            \n",
    "        return len(issues) == 0\n",
    "\n",
    "# Initialize and run environment detection\n",
    "env_detector = EnvironmentDetector()\n",
    "env_detector.detect_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dependencies_header"
   },
   "source": [
    "## üì¶ Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "class DependencyManager:\n",
    "    def __init__(self):\n",
    "        self.packages = {\n",
    "            'essential': [\n",
    "                'torch>=2.1.0',\n",
    "                'torchvision>=0.16.0',\n",
    "                'diffusers>=0.30.3',\n",
    "                'transformers>=4.44.2',\n",
    "                'accelerate>=0.33.0',\n",
    "                'xformers>=0.0.23',\n",
    "            ],\n",
    "            'video': [\n",
    "                'opencv-python-headless>=4.8.0',\n",
    "                'imageio[ffmpeg]>=2.31.0',\n",
    "                'moviepy>=1.0.3',\n",
    "                'av>=10.0.0',\n",
    "                'decord>=0.6.0'\n",
    "            ],\n",
    "            'ui': [\n",
    "                'streamlit>=1.28.0',\n",
    "                'gradio>=4.0.0',\n",
    "                'ipywidgets>=8.0.0',\n",
    "                'matplotlib>=3.7.0',\n",
    "                'pillow>=10.0.0'\n",
    "            ],\n",
    "            'utils': [\n",
    "                'huggingface-hub>=0.19.0',\n",
    "                'safetensors>=0.4.0',\n",
    "                'einops>=0.7.0',\n",
    "                'omegaconf>=2.3.0',\n",
    "                'pyngrok>=7.0.0'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def install_packages(self, category='all'):\n",
    "        \"\"\"Install packages with progress tracking\"\"\"\n",
    "        categories = [category] if category != 'all' else self.packages.keys()\n",
    "        \n",
    "        # Special handling for PyTorch with CUDA\n",
    "        if category == 'all' or category == 'essential':\n",
    "            print(\"\\nüî• Installing PyTorch with CUDA support...\")\n",
    "            torch_install_cmd = [\n",
    "                sys.executable, '-m', 'pip', 'install', 'torch>=2.1.0', 'torchvision>=0.16.0', 'torchaudio>=2.1.0',\n",
    "                '--index-url', 'https://download.pytorch.org/whl/cu121', '--quiet'\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(torch_install_cmd, capture_output=True, text=True, timeout=600)\n",
    "                if result.returncode == 0:\n",
    "                    print(\"‚úÖ PyTorch with CUDA installed successfully\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è PyTorch CUDA installation failed, trying CPU version...\")\n",
    "                    # Fallback to CPU version\n",
    "                    cpu_cmd = [sys.executable, '-m', 'pip', 'install', 'torch>=2.1.0', 'torchvision>=0.16.0', 'torchaudio>=2.1.0', '--quiet']\n",
    "                    subprocess.run(cpu_cmd, timeout=600)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"‚ö†Ô∏è PyTorch installation timeout, continuing...\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error installing PyTorch: {str(e)}\")\n",
    "                \n",
    "        for cat in categories:\n",
    "            if cat == 'essential':  # Skip essential as we handled PyTorch separately\n",
    "                packages = [p for p in self.packages[cat] if not p.startswith('torch')]\n",
    "            else:\n",
    "                packages = self.packages[cat]\n",
    "                \n",
    "            if not packages:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüì¶ Installing {cat} packages...\")\n",
    "            \n",
    "            progress_bar = tqdm(packages, desc=f\"Installing {cat}\")\n",
    "            for package in progress_bar:\n",
    "                try:\n",
    "                    progress_bar.set_postfix_str(f\"Installing {package.split('>=')[0]}\")\n",
    "                    result = subprocess.run(\n",
    "                        [sys.executable, '-m', 'pip', 'install', package, '--quiet'],\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        timeout=300\n",
    "                    )\n",
    "                    if result.returncode != 0:\n",
    "                        print(f\"‚ö†Ô∏è Warning: Failed to install {package}\")\n",
    "                        print(f\"Error: {result.stderr}\")\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    print(f\"‚ö†Ô∏è Timeout installing {package}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error installing {package}: {str(e)}\")\n",
    "                    \n",
    "        print(\"\\n‚úÖ Dependencies installation completed!\")\n",
    "        \n",
    "    def verify_installation(self):\n",
    "        \"\"\"Verify critical packages are installed\"\"\"\n",
    "        critical_imports = {\n",
    "            'torch': 'PyTorch',\n",
    "            'diffusers': 'Diffusers',\n",
    "            'transformers': 'Transformers',\n",
    "            'cv2': 'OpenCV',\n",
    "            'imageio': 'ImageIO',\n",
    "            'streamlit': 'Streamlit'\n",
    "        }\n",
    "        \n",
    "        print(\"\\nüîç Verifying installations...\")\n",
    "        for module, name in critical_imports.items():\n",
    "            try:\n",
    "                __import__(module)\n",
    "                print(f\"‚úÖ {name} - OK\")\n",
    "            except ImportError:\n",
    "                print(f\"‚ùå {name} - FAILED\")\n",
    "                \n",
    "        # Check CUDA availability\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"‚úÖ CUDA - OK (Device: {torch.cuda.get_device_name(0)})\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è CUDA - Not available\")\n",
    "        except:\n",
    "            print(\"‚ùå CUDA - Error checking\")\n",
    "\n",
    "# Install dependencies\n",
    "dep_manager = DependencyManager()\n",
    "dep_manager.install_packages()\n",
    "dep_manager.verify_installation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "models_header"
   },
   "source": [
    "## ü§ñ Step 3: Model Download & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_setup"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from pathlib import Path\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        self.models_dir = Path(\"/content/models\")\n",
    "        self.models_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.model_configs = {\n",
    "            'cogvideox': {\n",
    "                'repo_id': 'THUDM/CogVideoX-2b',\n",
    "                'local_dir': self.models_dir / 'CogVideoX-2b',\n",
    "                'size_gb': 8.5,\n",
    "                'description': 'Main video generation model'\n",
    "            },\n",
    "            'venhancer': {\n",
    "                'repo_id': 'jwhejwhe/VEnhancer',\n",
    "                'local_dir': self.models_dir / 'VEnhancer',\n",
    "                'size_gb': 2.1,\n",
    "                'description': 'Video enhancement model'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def download_model(self, model_name, force_download=False):\n",
    "        \"\"\"Download a specific model with progress tracking\"\"\"\n",
    "        if model_name not in self.model_configs:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "            \n",
    "        config = self.model_configs[model_name]\n",
    "        local_dir = config['local_dir']\n",
    "        \n",
    "        # Check if model already exists\n",
    "        if local_dir.exists() and not force_download:\n",
    "            print(f\"‚úÖ {model_name} already exists at {local_dir}\")\n",
    "            return str(local_dir)\n",
    "            \n",
    "        print(f\"\\nüì• Downloading {model_name} ({config['size_gb']:.1f}GB)...\")\n",
    "        print(f\"üìù {config['description']}\")\n",
    "        \n",
    "        try:\n",
    "            # Create progress callback\n",
    "            def progress_callback(block_num, block_size, total_size):\n",
    "                if total_size > 0:\n",
    "                    percent = min(100, (block_num * block_size * 100) / total_size)\n",
    "                    print(f\"\\rProgress: {percent:.1f}%\", end=\"\")\n",
    "                    \n",
    "            # Download with resume capability\n",
    "            local_dir_str = str(local_dir)\n",
    "            snapshot_download(\n",
    "                repo_id=config['repo_id'],\n",
    "                local_dir=local_dir_str,\n",
    "                resume_download=True,\n",
    "                local_dir_use_symlinks=False,\n",
    "                ignore_patterns=[\"*.git*\", \"README.md\", \"*.txt\"]\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚úÖ {model_name} downloaded successfully!\")\n",
    "            return local_dir_str\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error downloading {model_name}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    def download_all_models(self):\n",
    "        \"\"\"Download all required models\"\"\"\n",
    "        total_size = sum(config['size_gb'] for config in self.model_configs.values())\n",
    "        print(f\"\\nüì¶ Downloading all models (Total: {total_size:.1f}GB)\")\n",
    "        \n",
    "        downloaded_models = {}\n",
    "        for model_name in self.model_configs.keys():\n",
    "            path = self.download_model(model_name)\n",
    "            if path:\n",
    "                downloaded_models[model_name] = path\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Failed to download {model_name}\")\n",
    "                \n",
    "        return downloaded_models\n",
    "        \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get information about downloaded models\"\"\"\n",
    "        info = {}\n",
    "        for model_name, config in self.model_configs.items():\n",
    "            local_dir = config['local_dir']\n",
    "            info[model_name] = {\n",
    "                'exists': local_dir.exists(),\n",
    "                'path': str(local_dir),\n",
    "                'size_gb': config['size_gb'],\n",
    "                'description': config['description']\n",
    "            }\n",
    "        return info\n",
    "        \n",
    "    def cleanup_models(self):\n",
    "        \"\"\"Clean up downloaded models to free space\"\"\"\n",
    "        import shutil\n",
    "        if self.models_dir.exists():\n",
    "            shutil.rmtree(self.models_dir)\n",
    "            print(\"üóëÔ∏è All models cleaned up\")\n",
    "\n",
    "# Initialize model manager and download models\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Show model info\n",
    "print(\"üìã Model Information:\")\n",
    "for name, info in model_manager.get_model_info().items():\n",
    "    status = \"‚úÖ Downloaded\" if info['exists'] else \"‚ùå Not downloaded\"\n",
    "    print(f\"  {name}: {info['description']} ({info['size_gb']:.1f}GB) - {status}\")\n",
    "\n",
    "# Download all models\n",
    "downloaded_models = model_manager.download_all_models()\n",
    "print(f\"\\nüéâ Model setup complete! Downloaded {len(downloaded_models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_header"
   },
   "source": [
    "## üé¨ Step 4: Optimized Video Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "video_pipeline"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/models')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import gc\n",
    "from typing import List, Optional, Union\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Safe import of diffusers with compatibility check\n",
    "try:\n",
    "    from diffusers import CogVideoXPipeline, CogVideoXDPMScheduler\n",
    "    print(\"‚úÖ CogVideoX pipeline imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing CogVideoX pipeline: {str(e)}\")\n",
    "    print(\"üí° This might be due to version incompatibility.\")\n",
    "    print(\"üí° Please ensure you have:\")\n",
    "    print(\"   - torch>=2.1.0\")\n",
    "    print(\"   - diffusers>=0.30.3\")\n",
    "    print(\"   - transformers>=4.44.2\")\n",
    "    print(\"üí° Try running: pip install --upgrade torch diffusers transformers\")\n",
    "    raise\n",
    "\n",
    "# Version compatibility check\n",
    "print(f\"üìä Version Info:\")\n",
    "print(f\"   torch: {torch.__version__}\")\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"   diffusers: {diffusers.__version__}\")\n",
    "except:\n",
    "    print(\"   diffusers: version check failed\")\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"   transformers: {transformers.__version__}\")\n",
    "except:\n",
    "    print(\"   transformers: version check failed\")\n",
    "\n",
    "class OptimizedVideoPipeline:\n",
    "    def __init__(self, model_path: str, device: str = \"cuda\"):\n",
    "        self.device = device\n",
    "        self.model_path = model_path\n",
    "        self.pipeline = None\n",
    "        self.loaded = False\n",
    "        \n",
    "        # Optimized settings for 720p, 10s, 24fps\n",
    "        self.default_settings = {\n",
    "            'width': 720,\n",
    "            'height': 480,  # 3:2 aspect ratio for better model performance\n",
    "            'num_frames': 240,  # 10 seconds at 24fps\n",
    "            'fps': 24,\n",
    "            'num_inference_steps': 50,\n",
    "            'guidance_scale': 6.0,\n",
    "            'num_videos_per_prompt': 1\n",
    "        }\n",
    "        \n",
    "    def load_pipeline(self):\n",
    "        \"\"\"Load the video generation pipeline with memory optimization\"\"\"\n",
    "        if self.loaded:\n",
    "            return\n",
    "            \n",
    "        print(\"üîÑ Loading CogVideoX pipeline...\")\n",
    "        \n",
    "        try:\n",
    "            # Clear memory before loading\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Load pipeline with optimizations\n",
    "            self.pipeline = CogVideoXPipeline.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                variant=\"fp16\",\n",
    "                use_safetensors=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            self.pipeline = self.pipeline.to(self.device)\n",
    "            \n",
    "            # Enable memory efficient optimizations\n",
    "            self.pipeline.enable_model_cpu_offload()\n",
    "            self.pipeline.enable_vae_slicing()\n",
    "            \n",
    "            # Try to enable xformers if available\n",
    "            try:\n",
    "                self.pipeline.enable_xformers_memory_efficient_attention()\n",
    "                print(\"‚úÖ xFormers enabled for memory efficiency\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è xFormers not available, using default attention\")\n",
    "                \n",
    "            self.loaded = True\n",
    "            print(\"‚úÖ Pipeline loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading pipeline: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def generate_video(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        negative_prompt: str = None,\n",
    "        seed: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Generate a single video with optimized settings\"\"\"\n",
    "        if not self.loaded:\n",
    "            self.load_pipeline()\n",
    "            \n",
    "        # Merge settings\n",
    "        settings = {**self.default_settings, **kwargs}\n",
    "        \n",
    "        print(f\"üé¨ Generating video: {settings['width']}x{settings['height']}, {settings['num_frames']} frames, {settings['fps']} fps\")\n",
    "        print(f\"üìù Prompt: {prompt}\")\n",
    "        \n",
    "        # Set up generator for reproducibility\n",
    "        generator = None\n",
    "        if seed is not None:\n",
    "            generator = torch.Generator(device=self.device).manual_seed(seed)\n",
    "            print(f\"üé≤ Using seed: {seed}\")\n",
    "            \n",
    "        try:\n",
    "            # Clear memory before generation\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Generate video\n",
    "            with torch.inference_mode():\n",
    "                result = self.pipeline(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    width=settings['width'],\n",
    "                    height=settings['height'],\n",
    "                    num_frames=settings['num_frames'],\n",
    "                    num_inference_steps=settings['num_inference_steps'],\n",
    "                    guidance_scale=settings['guidance_scale'],\n",
    "                    num_videos_per_prompt=settings['num_videos_per_prompt'],\n",
    "                    generator=generator,\n",
    "                    output_type=\"pil\"\n",
    "                )\n",
    "                \n",
    "            # Extract frames\n",
    "            frames = result.frames[0]\n",
    "            \n",
    "            # Clean up\n",
    "            del result\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            print(f\"‚úÖ Generated {len(frames)} frames successfully!\")\n",
    "            return frames, settings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating video: {str(e)}\")\n",
    "            # Clean up on error\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            raise\n",
    "            \n",
    "    def save_video(\n",
    "        self,\n",
    "        frames: List[Image.Image],\n",
    "        output_path: str,\n",
    "        fps: int = 24,\n",
    "        quality: int = 8\n",
    "    ):\n",
    "        \"\"\"Save frames as video file\"\"\"\n",
    "        print(f\"üíæ Saving video to {output_path}...\")\n",
    "        \n",
    "        try:\n",
    "            # Convert PIL images to numpy arrays\n",
    "            video_frames = []\n",
    "            for frame in frames:\n",
    "                if isinstance(frame, Image.Image):\n",
    "                    video_frames.append(np.array(frame))\n",
    "                else:\n",
    "                    video_frames.append(frame)\n",
    "                    \n",
    "            # Save using imageio\n",
    "            with imageio.get_writer(\n",
    "                output_path,\n",
    "                fps=fps,\n",
    "                codec='libx264',\n",
    "                quality=quality,\n",
    "                pixelformat='yuv420p'\n",
    "            ) as writer:\n",
    "                for frame in video_frames:\n",
    "                    writer.append_data(frame)\n",
    "                    \n",
    "            print(f\"‚úÖ Video saved successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving video: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def unload_pipeline(self):\n",
    "        \"\"\"Unload pipeline to free memory\"\"\"\n",
    "        if self.pipeline:\n",
    "            del self.pipeline\n",
    "            self.pipeline = None\n",
    "            self.loaded = False\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"üóëÔ∏è Pipeline unloaded and memory cleared\")\n",
    "\n",
    "# Initialize the pipeline\n",
    "cogvideox_path = \"/content/models/CogVideoX-2b\"\n",
    "video_pipeline = OptimizedVideoPipeline(cogvideox_path)\n",
    "\n",
    "print(\"üé¨ Video pipeline initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examples_header"
   },
   "source": [
    "## üéØ Step 5: Example Usage & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_usage"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"/content/outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Example prompts optimized for realistic human poses\n",
    "example_prompts = [\n",
    "    \"A professional businesswoman walking confidently down a modern office hallway, natural lighting, 4K quality\",\n",
    "    \"A young man doing yoga poses in a peaceful garden, morning sunlight, cinematic composition\",\n",
    "    \"A dancer performing contemporary dance moves in an empty studio, dramatic lighting, artistic shot\",\n",
    "    \"An elderly person reading a book in a comfortable armchair by a window, warm lighting, cozy atmosphere\",\n",
    "    \"A chef preparing ingredients in a modern kitchen, professional cooking, dynamic movements\"\n",
    "]\n",
    "\n",
    "# Display example prompts\n",
    "print(\"üéØ Example Prompts for Realistic Human Poses:\")\n",
    "for i, prompt in enumerate(example_prompts, 1):\n",
    "    print(f\"\\n{i}. {prompt}\")\n",
    "\n",
    "# Quick test function\n",
    "def quick_test():\n",
    "    \"\"\"Quick test of the video generation pipeline\"\"\"\n",
    "    print(\"\\nüß™ Running quick test...\")\n",
    "    \n",
    "    try:\n",
    "        # Load pipeline\n",
    "        video_pipeline.load_pipeline()\n",
    "        \n",
    "        # Generate a short test video\n",
    "        test_frames, test_settings = video_pipeline.generate_video(\n",
    "            prompt=\"A person waving hello, friendly gesture, clear background\",\n",
    "            width=480,\n",
    "            height=320,\n",
    "            num_frames=48,  # 2 seconds at 24fps\n",
    "            fps=24,\n",
    "            num_inference_steps=25,  # Faster for testing\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Save test video\n",
    "        test_path = \"/content/outputs/test_video.mp4\"\n",
    "        success = video_pipeline.save_video(test_frames, test_path, fps=24)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"‚úÖ Test successful! Video saved to: {test_path}\")\n",
    "            \n",
    "            # Display test video\n",
    "            display(Video(test_path, width=480, height=320))\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Test failed: Could not save video\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Generate video function\n",
    "def generate_video_simple(\n",
    "    prompt: str,\n",
    "    negative_prompt: str = \"blurry, low quality, distorted, artifacts\",\n",
    "    width: int = 720,\n",
    "    height: int = 480,\n",
    "    duration: int = 10,\n",
    "    fps: int = 24,\n",
    "    seed: int = 42\n",
    "):\n",
    "    \"\"\"Simple function to generate a video\"\"\"\n",
    "    print(f\"üé¨ Generating video with prompt: {prompt}\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate frames\n",
    "        num_frames = duration * fps\n",
    "        \n",
    "        # Generate video\n",
    "        frames, settings = video_pipeline.generate_video(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            num_frames=num_frames,\n",
    "            fps=fps,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # Save video\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"/content/outputs/video_{timestamp}.mp4\"\n",
    "        \n",
    "        success = video_pipeline.save_video(frames, output_path, fps=fps)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"‚úÖ Video generated successfully: {output_path}\")\n",
    "            \n",
    "            # Display video\n",
    "            display(Video(output_path, width=min(width, 640), height=min(height, 480)))\n",
    "            \n",
    "            return output_path\n",
    "        else:\n",
    "            print(\"‚ùå Failed to save video\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating video: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Memory status function\n",
    "def check_memory_status():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            cached = torch.cuda.memory_reserved(0) / 1024**3\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            \n",
    "            print(f\"üß† GPU Memory Status:\")\n",
    "            print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "            print(f\"  Cached: {cached:.2f} GB\")\n",
    "            print(f\"  Total: {total:.2f} GB\")\n",
    "            print(f\"  Free: {total - cached:.2f} GB\")\n",
    "            \n",
    "        # RAM status\n",
    "        import psutil\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"\\nüß† RAM Status:\")\n",
    "        print(f\"  Used: {memory.used / 1024**3:.2f} GB\")\n",
    "        print(f\"  Available: {memory.available / 1024**3:.2f} GB\")\n",
    "        print(f\"  Total: {memory.total / 1024**3:.2f} GB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking memory: {str(e)}\")\n",
    "\n",
    "# Check memory status\n",
    "check_memory_status()\n",
    "\n",
    "print(\"\\nüéØ Ready for video generation!\")\n",
    "print(\"\\nüí° Usage Examples:\")\n",
    "print(\"  quick_test()  # Run a quick test\")\n",
    "print(\"  generate_video_simple('A person dancing in the rain')  # Generate a video\")\n",
    "print(\"\\nüìÅ All videos will be saved to: /content/outputs/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}